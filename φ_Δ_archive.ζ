#!/usr/bin/env ζ
# config: os_daemon(φ`~/code/scratch/φ_Δ_archive.ζ`).restart()

# notify('φ_Δ_archive start')

#################################### prelude ###################################
Δ_as ← (s,f)=>{t←;ι←; last ← @device0('watcher_'+simple_hash([s,f])); if(!( last.ι===(t= (ι=s())|>(f)|>(simple_hash2)) )){ last.ι = t; ↩ ι } }

################################## φ_Δ_archive #################################

# ! slowing down machine ?
# # this is not quite right. like, the thing we're pretending to be is a giant stream of all filesystem events. filtering through that by a filesystem set selector is outside the scope of EventEmitter.
# # it probably ought to be using reactive programming?

# # huh, we could/should implement it as a library that logs all (requested) filesystem events whether or not its caller is running ?
# # that would require reliable long-running processes

# log2 ← (…a)=> φ`/tmp/log.φ_Δ_archive`.text += a.join(' ')+'\n'

# fsʷ ← new node.EventEmitter().on('newListener',λ(fl,f){
# 	if (fl !== (fl=φ(fl)+'')){ fsʷ.on(fl,f); process.nextTick(=> fsʷ.removeListener(fl,f) ); ↩ }
# 	fsʷ._events[fl] || npm`chokidar@2.0.0`.watch(fl,{ ,ignoreInitial:✓ ,alwaysStat:✓ })
# 		.on('error',λ(ι){ console.error('[fs_watch] wat '+ι) })
# 		.on('all',λ(ev,ι,stat){ t ← {'addDir':'add','unlinkDir':'unlink'}; type ← t[ev]||ev; fsʷ.emit(fl, ι, {type, dir: !!t[ev], stat, time: type==='unlink'? new Date() : stat.ctime}) })
# 	})

# φ.cwd = φ`~/file/notes`
# fsʷ.on('.',(fl,{type,dir,time})=>{ if( dir || fl.re`(^|/)\.archive(/|$)` || fl.re`\.(?:gif|jpg|mov|mp3|mp4|pages|pdf|png|wav)$` ) ↩
# 	log2( ,time.ymdhmss,'#note',type.padEnd(6),fl )
# 	# ! ? φl ← φ(fl,{exists:type!=='unlink'}); φ`.archive/${time.ymdhmss} ${φl.exists?'=':'X'} ${fl}`.buf = φl.buf
# 	φ`.archive/…${fl==='.auto/Local/Auto Save Session.sublime_session' && '.sublime/'}${time.ymdhmss} ${type==='unlink'?'X':'='} ${fl}`.buf = type==='unlink'? '' : φ(fl).buf
# 	})

# # bug: this only gets subfiles if they were new since the last version *and* their folders were not



# this is pretty fucked up

# ########## so let's add another fucked up bit, and watch for doi files #########
# E.has_doi = φι=>{
# 	is_doi ← re`(doi:|doi\.org/) *…${npm`doi-regex@0.1.4`().source}`.i
# 	↩ Π((yes,no)=> !φι.match(/\.pdf$/i)? yes(✗) : npm`textract@2.1.1`.fromFileWithPath(φι,(e,ι)=> e? no(e) : yes(is_doi.test(ι)) ) ) }
# npm`chokidar@2.0.0`.watch(φ`~/Downloads`+'',{depth:0,alwaysStat:✓}).on('all',λ(ev,ι){
# 	if (φ(ι).φ`..`+''===φ`~/Downloads`+'') {
# 		has_doi(ι).then(t=>{ if (t) {
# 			hsᵥ`hs.alert(${'moving paper '+φ(ι).name+' to mendeley'})`
# 			node.fs.rename(ι,φ`~/Documents/Mendeley Desktop/${φ(ι).name}`+'')
# 			} })
# 		} })

############# so let's add another fucked up bit, and kill DS_Store ############
npm`chokidar@2.0.0`.watch(φ`~/Downloads`+'',{depth:0,alwaysStat:✓}).on('all',λ(ev,ι){
	log2( ,Time().ymdhmss,'#DS_Store',ι )
	if (φ(ι).φ`..`+''===φ`~/Downloads`+'')
	if (φ(ι).name === '.DS_Store')
		shᵥ`rm ${ι}`
	})

########################### look at me cron no hands ###########################
;(=>{
	# u ← 'crazyblogging'
	# this will work fine for crazyblogging but Δ_as ... isnt reentrant. #fixme
	u ← 'nihilsupernum'
	page ← `https://${u}.tumblr.com/`
	t ← Δ_as(=> GET_L(page), ι=> (ι+'').match(re`\b(?:(https?|chrome|chrome-extension)://|(?:file|mailto):)(?:[^\s“”"<>]*\([^\s“”"<>]*\))?(?:[^\s“”"<>]*[^\s“”"<>)\]}⟩?!,.:;])?`.g) .filter(ι=>{ ι=npm`urijs@1.18.12`(ι); ↩ ι.hostname()+'/'+ι.segment(0)===`${u}.tumblr.com/post` }) )
	if( t ) notify(`Δ tumblr.${u}`).then(=> go_to(page))
	}).every(0.01d)

################################### yahh cron ##################################
;(=>{
	t ← φ`~/code/scratch/daily.sh`+''
	node.child_process.spawn(t,{,shell:✓,detached:✓,stdio:'ignore'}).unref()
	}).every(1.1d)

################################## cronomatron #################################
;(=> φ`~/file/pix/→ bg laptop`.TMP_children() ) ≫(ι=> 🎲(ι) |>(imgur_from).then(ι=> set_newtab_bg(ι.link)))
.every(0.01d)
