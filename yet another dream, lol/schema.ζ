/*#######################*/ global['try #1'] = _.once(=>{ ######################
parse_bash(Ï†`~/.bashrc`.text).then(Î¹=> global.train = Î¹)

schema2 = Î¹=>{ ð…ªð…žð…‚ð…‚ð…œâ†;
	# sc_merge â† Î»(a,b){ak â† _.keys(a); bk â† _.keys(b)
	# 	bk.-(ak).forEach(k=> a[k] = b[k])
	# 	ak.âˆ©(bk).forEach(k=> a[k] = !Tprim(a[k])? sc_merge(a[k],b[k]) : !Tprim(b[k])? 'error' : a[k])
	# 	â†© a }

	schemas â† (=>{ # uniq by â‰ˆ.schema , index by .Î¹
		as_schema â† Î¹=> _(Î¹).mapObject(Î¹â‡’
			: T.âœ“âœ—(Î¹)? âœ“
			: Tstr(Î¹)? ''
			: Tnum(Î¹)? 0
			: schemas.get_Î¹(Î¹)? schemas.get_Î¹(Î¹).id : â€½(Î¹) )
		sch_to_data â† new Map()
		Î¹_to_sch â† new Map()
		â†© {
			,get_Î¹: Î¹=> sch_to_data.get(Î¹_to_sch.get(Î¹))
			,add_Î¹: Î¹=>{
				schema â† as_schema(Î¹)
				sch_k â† simple_flesh(schema)
				Î¹_to_sch.set(Î¹,sch_k)
				sch_to_data.has(sch_k) || sch_to_data.set(sch_k,{ Î¹:schema, id:'#'+([#Q (ð…ªð…žð…‚ð…‚ð…œ||(ð…ªð…žð…‚ð…‚ð…œ= [0] )).0 #Q].Î¹++).toString(36), })
				}
			,map: f=> [â€¦sch_to_data.values()].map(f)
			# ,simplify: =>{
			# 	...
			# 	}
			,show(){
				d_t_s â† sch_to_data.â»Â¹declare_uniq
				s_t_Î¹ â† Î¹_to_sch.â»Â¹
				â†© @.map(Î¹=> JSON_pretty(Î¹.Î¹)+' '+s_t_Î¹.get(d_t_s.get(Î¹)).â€–+' '+Î¹.id ) }
			} })()

	walk(Î¹,Î¹=> Tprim(Î¹) || schemas.add_Î¹(Î¹) )

	â†© schemas.show() }

if( global.train ) sb.tab.push(schema2(train))

# summarizes ~1200 objects with ~500 types
# could try to simplify
/*################################*/ }); /* ####################################
# make pseudocode instead

node â† js.object
	using: node -> schema â† _(Î¹).mapObject(Î¹â‡’
		: T.âœ“âœ—(Î¹)? âœ“
		: Tstr(Î¹)? ''
		: Tnum(Î¹)? 0
		: @.as.name(Î¹) )
schema
	using â‰¡ â† simple_flesh
	using [node -> schema]â»Â¹
schema â€¦â†> name : name = `#${[schema age index] as base 36}`

show(){â†© schemas(Î»(){â†© for @.schema: (JSON_pretty(Î¹) @.as.node(Î¹).â€– @.as.name(Î¹)).join(' ') }) },

# simplify
node -> schema â† for range: if Î¹.as.(node).â€– = 1 and Î¹.[* - T.prim]/and*.as.(node).â€– = 1: Î¹.[* - T.prim] â† *.as.schema

;(async =>{
	train â† await parse_bash(Ï†`~/.bashrc`.text)
	sb.tab.push( schemas(Î»(){â†© @(train).as.schema }) )
	})()

###################### */ global['try #2'] = _.once(=>{ ######################
global['support #1']()
# compile pseudocode to js_lang

Type('node',{
	,'âˆˆ':Object
	})
Type('schema',{
	,'â‰¡':simple_flesh
	# ,representative(){ _(@).max(.precedence) }
	,show(){â†© [ JSON_pretty(@), @.as('(node)').â€–, @.as.name, ].join(' ') }
	})
node.->.schema = Î»(Î¹){â†© _(Î¹).mapObject(Î¹â‡’
	: T.âœ“âœ—(Î¹)? âœ“
	: Tstr(Î¹)? ''
	: Tnum(Î¹)? 0
	: @(Î¹).as.name ) },
schema.â€¦â†>.name = Î»(Î¹){â†© `#${age(Î¹).indexâˆˆ(age(@)).as.base(36)}` }

# simplify
# the way that all these concepts interlock is. fucky.
# im too sleep to know how to fix this.
# u do it

schema.â‰¡.close_over!(Î¹=>{


	q_isâ»Â¹ â† .as('(node)').â€–===1
	names â† Î¹.filter(.is.name)

	# should be:
	# if your children names are contained by only one schema:
	# 	fold them into you
	# good skill!

	â†© q_isâ»Â¹(Î¹) && names.map(Î¹=> q_isâ»Â¹)./(and)
		? _({}) â€¦â† ( Î¹, names.map(.as.schema) )
		: Î¹ } )

;(async =>{
	sb.tab.push( node(await parse_bash(Ï†`~/.bashrc`.text)).as.schema )
	})()

/*################*/ }); global['support #1'] = _.once(=>{ ###################
and â† (a,b)=> a && b

# missing:
./
.map
.filter

.is.typename
.as.typename
.as('(typename)')
Type
	Type(typename,{
		'âˆˆ':
		[typename.->.typename]:Î»(Î¹){â†© ...}
			@(Î¹).as.typename
		'â‰¡':
			.close_over!
		[Any]:
		show:Î»(){â†© ""}
		})
		.->
			.â»Â¹
		.â€¦â†>
Any
	[Any]:
.â€–
	(Array|Set|Map|etc).prototype.â€–
age
	age(Î¹ âˆˆ typename)
		.indexâˆˆ
	age(typename)
		.as
			.base(int)

/*##############################*/ }) /* note ##################################
# â€¡ alice, expand schema to handle parse_bash output

# sc_merge â† Î»(a,b){ak â† _.keys(a); bk â† _.keys(b)
# 	bk.-(ak).forEach(k=> a[k] = b[k])
# 	ak.âˆ©(bk).forEach(k=> a[k] = !Tprim(a[k])? sc_merge(a[k],b[k]) : !Tprim(b[k])? 'error' : a[k])
# 	â†© a }

it looks like you're trying to write a clustering algorithm on the semantic structure of arbitrary json documents
why are you doing this.
i mean it sounds cool but isnt this a major addition at this stage?
what does it do?

in other words, you're trying to write a program that makes guesses at how to parse arbitrary json you found on the internet
why
this is.

parser inference = type inference

but you dont know much about inference really



so if i am?

this is a, um
this is part of the obbbbject inspector html thing?
huh
maybe
i also like schemas and i think this sort of work transfers

