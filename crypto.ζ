#!/usr/bin/env ζλ

# consider buffer
# consider vectorlang

# pal::a.b = `https://cryptopals.com/sets/${a}/challenges/${b}`

# @pal::1.1 @<pre> ι0 as base64 = ι1

# @pal::1.2 @<pre> ι0 xor ι1 = ι2

# @pal::1.3
# [@<pre>] = msg xor byte

# msg: guess by ×-avg [ι * [wikipedia::cat text freq normalize]] (?laplacian prior?) (?update syntax?)
# msg: guess by [wikipedia::cat text freq]


# @pal::1.4
# @md`[this file](↩)`
# find [= msg xor byte]

# @pal::1.5
# @<pre> ι0 xor [repeat "ICE"] as hex = ι1

# @pal::1.6
# edit_distance(a,b) = [a,b]∈binary xor /[=1] .‖
#     edit_distance …[@<pre>] = 37
# ciph = @md`[There's a file here](↩)`
# ciph = msg xor [repeat key]
# assume key.‖ ⊂ 2..40




# --------------------------------------------------------------------------------
# so ive got, um, known alphabet frequencies of a corpus
# and ive got some strings i'm thinking of unsorted letters and i'm trying to figure out-

# ------------------------------
# well, i've got a corpus
# im imagining it was generated by randomly selecting letters according to a set of fixed frequencies
# lets compute my distribution over those fixed frequencies based on the corpus
# my prior is flat

# also ive got some strings from somewhere, lets interpret them in the same way i guess?

# and compute my distributions over those fixed frequencies too, if my prior is like - wait, no

# im trying to compute if it's, like, probably the same *stuff* as the corpus
# or im trying to compute the degree to which it looks like the same stuff as the corpus

# ------------ well. -----------
# i have a text, call it if_is_similar_to
# i have some more texts and i want to find out which ones are most similar to if_is_similar_to

# lets do something simple
# interpret the texts as random strings of letters generated according to a set of fixed frequencies

# if gen:if_is_similar_to is a small norm distance from gen:text then it is probably more similar

# if we keep our prior flat, we can find distributions for gens of texts

# if we look for the probability that texts have the *same* generator thats an integral of the product of the generator distributions (right?)
# if our prior over generator distribution space is *also* flat

# luckily all our generator distributions are n-spheres *so* that probability will be 

# wow this is so wrong

# ------------------------------
# aaa?

# looks like its time to sleep

# ------------------------------
# msg: guess by [wikipedia::cat text freq]
# so uh how can i do this
# i want to produce a probability that this is an english sentence, given simple priors
# scoring "abc" worse than "abcabcabcabcabcabc^abc"

# ------------------------------
# well, ive got some short texts

texts ← []; text ← ι=> texts.push(…ι.trim().split(/\n+/).map(ι=> ι.replace(/#.*/g,'').trim()))

text(`
1110100010101100001000100100000000110001100101001011000010100110010001101000010001101010111010010000 # 100..map(=> random()>0.5? 0 : 1).join('')+' # '+code
0001011111010011110001100011101011111011101010111111000001101111100101011111011111001100110010101100 # 100..map(=> random()>0.6? 0 : 1).join('')+' # '+code
1100011101101100011111010
1110101110111111111111011010101111101100001011011110000011011111100111111011110101010101011110011111 # 100..map(=> random()>0.7? 0 : 1).join('')+' # '+code
`)
# also i have a longer text
text(`
01011110110101100111010111111010101101000000000111011110111111101000111011011111001010111101101111000010010000100110001110010111011100111101111100111100111100110010011100101111010010011010010010000001111011110011100110010100110111111010101000101001010110000110111101010101111011001101000110111011000011111001001001111001111010000010100100101110000110100010101000101011100000111000010100001010011101011101011111101101111101111101101101101101111011101111101111101100101010010110111101100111101000101110111001001011111101001101111100111111101110010101101100111111010100001001110111101011110110110100101001101111111000111010101110101110110111011111011101011110110001111001001010011111111100011111110101101110011100101101101011011111000111011011010111000110111110101110011100110100110001011111010101110011101111010111110011011111111011001110110101010010001011011010100000110101010111101011111111101101001111011111111011110111111011110110110101111101111111100001100111001111101101101010001011101101111101111001111111011110100010010001011100010110011001111101001011010101111010100001101100011011110111001011111011010100000010110110100001110111101100011100100010110010110001100011110111101000100101100110111111010100101111101101100101111000110011011111100011010100010111101110100111111100111110000110010001111101111111100010011100101000010000000111011000000101111011111100111111011111001000100111110111011101111111011110101000101001101001011111010101011101111001011111111111110010111111101111111100011011111111101111110100100111111111101011110101000111111100111101110101001001101101101111111110101101011111011101101111011111101100000101000111100010110100111100100101001110111110100101100010111010110011111011000011000101111000101001101011100101111101110010001011000001001100101110011111010110111110001011111011110010100101100110101111011000101110100111100101011001000000001111111011101110010010111001011111010001011011001100111111101111111011101010000111111011011111111011001111010111101110101111111101111101
11011111101111101001101101111101111111010111010101011011111101111011111010100001111010111011001111111111111101111110111111011100101111110010111111110110111011111110011101011111011110000111111111111111101111101111111001011100100111111110111111111110011110001111001111011100100000011011011110010111111111001111011011110001001001100111101101011110001111001101001100001111000010111111111101010111111111010011011110111011101111011111111111110101101110010100101110100110110110110111111100011100111110110111011111111110101110011101111011111111101011001111111110111110001110111111010111110001011011110111110111110111111101110111000011000111111011101111110111101110111110100111010010111100010011110111111100110110111101101101111101101111111110011111001101101111111110001101110100010101111011110100111101001111110110111011111111111101111111111111100110110111111110011111101000001011011001000111101110101011111111110101010010110100100111111110011111000111101100100111111111101111110111100011011111010101111011010110101100001010111110111001111011111111101010110011001111111001110110000011111111111001101011111111110111111111110101111011111111111110111000100011111111110010011110011000101101111010111111111011101111000111011011111111011101100101100111110011111011100111111111110111101100001100111111001101101111101011011101111000110001101100111110111111111010111011000111001101110011111001000100111111111001100110111000101111010111101111111101111011110110011111111101110010111111100111101111100001101101011011111111101000111010110111111011110110110111101111111110111101101011011111100010111111001110111110111101111111001111101101011110011111011101100011111010111111110111101010010111111111011110101111111100011111011111111011001101011100111111111011111001110110100110010101011101111111111111000100110111111111111110111111110110100110110101110101010101010101010111111001111001111001111111110110011110010111101101101110010110111101010001011000111101111111111001110111101101110011101101111101111111110111011011111001
`)
# im imagining the texts are generated by randomly selecting from (0 1) according to a set of fixed frequencies (which can be summarized as `p`)
# i want to find out which short texts are most similar to the longer text
# looking at each text with a flat prior,

cn.log(  texts.map(ι=>{ t ← _(ι).countBy(); k ← t[1]; n ← t[0]+t[1]; ↩ (k+1)/(n+2) })  )

# ok but i want to do this the full way

fact ← memoize_proc( ι=> ι===0? 1 : ι*fact(ι-1) )
# p(010) given p0=0.6 p1=0.4
0.144 # ( 0.6*0.6*0.4 )+' # '+code
# p(k0=2,k1=1,n=3) given p0=0.6 p1=0.4
0.216 # ( 0.6*0.6*0.6 * fact(3)/(fact(3)*fact(0)) )+' # '+code
0.43199999999999994 # ( 0.6*0.6*0.4 * fact(3)/(fact(2)*fact(1)) )+' # '+code
0.28800000000000003 # ( 0.6*0.4*0.4 * fact(3)/(fact(1)*fact(2)) )+' # '+code
0.06400000000000002 # ( 0.4*0.4*0.4 * fact(3)/(fact(0)*fact(3)) )+' # '+code

# https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair
# https://hplusminus.tumblr.com/post/158605421890/practical-bayesianism-the-sunrise-problem-and
# https://en.wikipedia.org/wiki/Bayesian_inference
# https://stackoverflow.com/a/3015044/322866
# https://en.wikipedia.org/wiki/Dirichlet_distribution
# https://mathematica.stackexchange.com/questions/42652/uniformly-distributed-n-dimensional-probability-vectors-over-a-simplex
# file:/~/books/Bayesian%20Data%20Analysis.pdf

# the things jimmy sent me that i am looking at are fascinating, but i dont really understand them
# http://emina.github.io/rosette/
# http://emina.github.io/rosette/apps.html
# http://github.com/emina/rosette
# http://github.com/Z3Prover/z3
# http://saturn.stanford.edu/pages/overviewindex.html
# http://theory.stanford.edu/~arbrad/papers/Understanding_IC3.pdf
# http://google.com/search?q=model+checking+algorithm+IC3

# https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical.2Fmultinomial
# is totally the thing you want to be doing here
