#!/usr/bin/env ζ
// a,b

// offset vector
// scale number of whichever a,b is lower-res

// ...



// try a as lower-res
// try b as lower-res

// ---------- to try ι ----------
// let a,b = other,ι

// for a given scale number,
// 	offset vector ranges for all overlap positions where at least one image is at least 30% over the other

// try scale numbers for all pixel sizes up to 10000

// utility is just, for the overlapping portion, what is the average pixel difference?

// find the global maximum


// unfortunately, this algorithm takes about a million times too long to run on my laptop
// let’s edit “average pixel difference” to be a random set of pixels, sampling 1/1000 pixels
// and let’s take a 

im_size ← ι => (shᵥ`identify -format '%w %h' ${ι}`+'').split(' ').map(parseInt.X)
gen_upscale ← λ(fl,L){ shᵥ`convert ${fl} -filter Lanczos -resize ${L}x${L} ${fl}_/${('0000'+L).slice(-4)}.png` }
score ← λ([a,b],iters){
	T ← λ({xₒ,yₒ,X,Y}){↩ [[xₒ,yₒ],[xₒ+X,yₒ+Y]]}
	intersection ← λ(a,b){↩ [[max(a[0][0],b[0][0]), max(a[0][1],b[0][1])], [min(a[1][0],b[1][0]), min(a[1][1],b[1][1])]]}
	t ← intersection(T(a),T(b)); t = _.zip(…t)
	// t = _.range(…t[0]).mapcat(x => _.range(…t[1]).map(y => {aₜ ← (y-a.yₒ)*a.X + (x-a.xₒ); bₜ ← (y-b.yₒ)*b.X + (x-b.xₒ); ↩ abs(a.ι[aₜ*3+0] - b.ι[bₜ*3+0]) + abs(a.ι[aₜ*3+1] - b.ι[bₜ*3+1]) + abs(a.ι[aₜ*3+2] - b.ι[bₜ*3+2])})); ↩ t.reduce(λ(a,b){↩ a+b}) / (t.length*3)
	// optimization!
	d←0; r ← 0; for(i←0;i<iters;i++) {x ← rand(t[0][0],t[0][1]); y ← rand(t[1][0],t[1][1]); d++;d%=3; aₜ ← (y-a.yₒ)*a.X + (x-a.xₒ); bₜ ← (y-b.yₒ)*b.X + (x-b.xₒ); r += abs(a.ι[aₜ*3+d] - b.ι[bₜ*3+d])}; ↩ r / iters }

var [to_scale,other] = ['a.jpg','b.jpg']
t ← max(…im_size(to_scale)); scales ← _.range(t, 10000).filter(ι => ι < t*1.6)

// _.range(3072,10000).map(gen_upscale.P(to_scale))

other_d←; pixels.read(other,'rgb',λ(e,ι){other_d = ι; cn.log('loaded other_d')})
scales_d ← {}; fetch_scale_d ← λ(ι,cb){t ← to_scale+'_/'+('0000'+ι).slice(-4)+'.png'; cn.log('loading',t); pixels.read(t,'rgb',λ(e,r){scales_d[ι] = r; cb.in(0)})}

rand_point_in_space ← λ(){
	Y ← rand(scales); xₒ ← rand(-20,91+1); yₒ ← rand(-295,-199+1)
	↩ [{xₒ,yₒ,Y}._.assign(scales_d[Y]), {xₒ:0,yₒ:0}._.assign(other_d)] }
rand_point_near_point ← λ(ι){t ← ι[0]
	Y ← min(scales[-1], max(scales[0], t.Y + rand(-1,1+1)))
	↩ [{xₒ: t.xₒ + rand(-1,1+1), yₒ: t.yₒ + rand(-1,1+1), Y}._.assign(scales_d[Y]), ι[1]] }

ptop ← ()=> top.map(ι => ι[0][0]._.omit('ι')._.assign({score:round(ι[1]*1e2)/1e2}))
top ← 20..map(()=> [,Infinity]); i←0; iters ← 1000; stop ← false; (λ Λ(){i++; if (stop) ↩;
	if (i % 200 === 0) {iters += 20; top = top.map(ι => [ι[0], score(ι[0], iters)])._.sortBy(1)}
	do {ι ← rand()>1/sqrt(i) && top.every(ι => ι[0])? rand_point_near_point(rand(top)[0]) : rand_point_in_space()}
	while (top.some(t => t.xₒ===ι[0].xₒ && t.yₒ===ι[0].yₒ && t.Y===ι[0].Y))
	if (!ι[0].ι) fetch_scale_d(ι[0].Y,rst); else tok = rst.in(0)
	λ rst(){ ι[0].ι || ι[0]._.assign(scales_d[ι[0].Y]); t ← [ι, score(ι,iters)]; if (t[1] < top[-1][1]) {top[-1] = t; top = top._.sortBy(1)}; if (t === top[0]) cn.log([i, ι[0]._.omit('ι'), round(t[1]*1e2)/1e2]); Λ.in(0) }
	}).in(0),0

// stop = true

// score should return a distribution, not just the mean. and then we can see that we’ve done a thousand calls to this function, so only its 99.8% interval should be *reliable* for a min
