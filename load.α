//#!/usr/bin/env node

fs ← require('fs')

minimist ← require('minimist')
sync ← require('sync')
_ ← require('underscore')

//__sub__ ← λ{__get__(v,b)}
__slice__ ← λ(v,a,b){return is(b)? v.slice(a,b) : v.slice(a)}
__sliceλ__ ← λ(a,b){return is(b)? λ{v[a:b]} : λ{v[a:]}}

print ← console.log.bind(console)
running_as ← process.argv₁.match(~/[^\/]*\/[^\/]*$/)₀
print('--- running as:',running_as,'---')

//===---------------------------===// utils //===--------------------------===//

seq ← λ{v isa Array? v : typeof(v)='string'? v.split('') : Object.keys(v).map(λ(k){return [k,vₖ]})}
Array.prototype.object ←! λ{this.reduce(λ(r,v){r[v₀] ←! v₁; return r},{})}
memoize_o ← λ(o,f){return λ(v){r ← own(o,v); return is(r)? r : (oᵥ ←! f(v))}}
merge_o ← λ{$args.map(λ{seq(v)}).m_concat().object()}

pad_left ← λ(v,s,l){return s.repeat(l-v.length)+v}
hex ← λ(v,l){return pad_left(v.toString(16),'0',l)}
now ← λ{Date.now() / 1000}
ord ← λ{v.charCodeAt(0)}
chr ← λ{String.fromCharCode(v)}
own ← λ(o,m){return Object.prototype.hasOwnProperty.call(o,m)? oₘ : undefined}
err ← λ(){print.apply(console, ['#error#:'].concat($args)); throw(Error())}
extend_function ← λ(f){r ← λ(){r ← f(); r.__proto__ ←! λ.prototype; return r}; r.prototype.__proto__ ←! Function.prototype; return r}
delset ← λ(o,m,v){if (¬is(v)) delete(oₘ); else oₘ ←! v}
is ← λ{v≠undefined}
i ← λ{parseInt(v)}
String.prototype.repeat ←! λ{v≤0? '' : new Array(v+1).join(this)}
Array.prototype.m_concat ←! λ{Array.prototype.concat.apply([],this)}
Object.defineProperty(Array.prototype,'no',{get:λ{this.length=0}})
Array.prototype.cartesian ←! λ{this.reduce(λ(a,v){r ← []; a.map(λ(a){seq(v).map(λ(b){r.push(a.concat([b]))})}); return r}, [[]])}
genex_2a ← λ(re){return _.range(2).map(λ{[128,128].slice(v).map(λ{_.range(v).map(λ{String.fromCharCode(v)})}).cartesian().map(λ{v.join('')})}).m_concat().filter(λ{v.match(re)})}

//===-----------------------===// lexing tools //===-----------------------===//

reader_or ← extend_function(λ{λ(start,s,line){return (λ[s₀]? λ[s₀](start+s₀,s[1:],line) : undefined) || (λ['']? λ[''](start,s,line) : undefined)}})
reader_or.prototype.get ←! λ(s){return this[s₀]? (this[s₀] isa reader_or? this[s₀].get(s[1:]) : this[s₀]) : undefined}
reader_or.prototype.set ←! λ(ss,r){
	seq(ss).map(λ(s){
		c ← s=''? '' : s₀; s ← s[1:]
		if (s ≠ '') (this[c] isa reader_or? this[c] : this[c] ←! new reader_or().set([''],own(this,c))).set([s],r)
		else delset(this,c,r)
	}.bind(this)); return this}
reader_or.prototype.reduce_l ←! λ(s){r ← []; line ← 1; while (s ≠ '') {t ← this('',s,line); r.push(t₀); s ←! t₁; line ←! t₂} return r}
reader_or.prototype.reduce   ←! λ(s){r ← [];           while (s ≠ '') {t ← this('',s     ); r.push(t₀); s ←! t₁            } return r}

//===------------------------===// characters //===------------------------===//

js_valid_symbol ← new (λ(){
	//! _short should be short
	_short ← ")0 !1 @2 #3 %5 ^6 &7 *8 (9 ∀A 'b {B ,C :c .d =E →e ≥ge >gt `k [L ≤le <lt -m ¬n ≠N |o +p ?q …r /s ~t ←w ǂǂ".split(' ').concat([' _']).map(λ{[v₀,v[1:]]})
	encode_short ← _short.map(λ{[v₀,'ǂ'+v₁]}).object()
	decode_short ← _short.map(λ{[v₁,v₀]}).object()
	is_start ← memoize_o(seq(';ǂ \t\n\x0b\x0c\x0d').map(λ{[v,false]}).object(),λ(v){try {eval('var '+v )} catch (e) {return false} return true})
	is_part  ← memoize_o(seq(';ǂ \t\n\x0b\x0c\x0d').map(λ{[v,false]}).object(),λ(v){try {eval('var a'+v)} catch (e) {return false} return true})
	encode_char ← memoize_o(encode_short, λ(v){return is_part(v)? v : 'ǂu'+hex(ord(v),4)})

	keywords ← ['break','do','instanceof','typeof','case','else','new','var','catch','finally','return','void','continue','for','switch','while','debugger','function','this','with','default','if','throw','delete','in','try','class','enum','extends','super','const','export','import','implements','let','private','public','yield','interface','package','protected','static']

	decoder ← new reader_or()
	decoder.set([''],λ(_,s){return [s₀,s[1:]]})
	decoder.set(['ǂu'],λ(_,s){return [chr(parseInt(s[:4],16)),s[4:]]})
	seq(decode_short).map(λ(kv){k ← kv₀; v ← kv₁; decoder.set(['ǂ'+k],λ(_,s){return [v,s]})})

	this.is_part ←! is_part
	this.encode ←! memoize_o(keywords.map(λ{[v,'ǂ'+v]}).object(),λ(v){r ← seq(v).map(encode_char).join(''); return (¬is_start(r₀)?'ǂ':'')+r})
	this.decode ←! memoize_o(keywords.map(λ{['ǂ'+v,v]}).object(),λ{decoder.reduce(v₀='ǂ'? v[1:] : v).join('')})
	})
unicode ← λ(table){return {
	subscripts:   table.map(λ{v₀}).filter(λ{v≠'_'}),
	midscripts:   table.map(λ{v₁}),
	superscripts: table.map(λ{v₂}).filter(λ{v≠'_'}),
	subscript:   λ{table.map(λ{[[v₁,v₀], [v₂,v₀]]}).m_concat().filter(λ{v₁≠'_'}).object()ᵥ},
	midscript:   λ{table.map(λ{[[v₀,v₁], [v₂,v₁]]}).m_concat().filter(λ{v₁≠'_'}).object()ᵥ},
	superscript: λ{table.map(λ{[[v₀,v₂], [v₁,v₂]]}).m_concat().filter(λ{v₁≠'_'}).object()ᵥ}
	}}('₀0⁰ ₁1¹ ₂2² ₃3³ ₄4⁴ ₅5⁵ ₆6⁶ ₇7⁷ ₈8⁸ ₉9⁹ ₊+⁺ ₋-⁻ ₌=⁼ ₍(⁽ ₎)⁾ ₐaᵃ _bᵇ _cᶜ _dᵈ ₑeᵉ _fᶠ _gᵍ ₕhʰ ᵢiⁱ ⱼjʲ ₖkᵏ ₗlˡ ₘmᵐ ₙnⁿ ₒoᵒ ₚpᵖ ᵣrʳ ₛsˢ ₜtᵗ ᵤuᵘ ᵥvᵛ _wʷ ₓxˣ _yʸ _zᶻ _Aᴬ _Bᴮ _Dᴰ _Eᴱ _Gᴳ _Hᴴ _Iᴵ _Jᴶ _Kᴷ _Lᴸ _Mᴹ _Nᴺ _Oᴼ _Pᴾ _Rᴿ _Tᵀ _Uᵁ _Vⱽ _Wᵂ'.split(' '))

//===--------------------------===// Symbol //===--------------------------===//

// Symbol ← λ(name,space,line){this.name ←! name; this.space ←! space; if (line) this.line ←! line}
// Symbol.prototype.toString ←! Symbol.prototype.inspect ←! λ{'`'+this.space₀+this.name+this.space₁}
// Symbol.prototype.with_name ←! λ{new Symbol(v,this.space,this.line)}
// S ← λ{new Symbol(v,'₋₋',b)}

//===---------------------===// load.α hack utils //===--------------------===//

pr ← λ(){if (running_as = 'bin/load.js') print.apply(this,['##'].concat($args)); return $args₀}
printable ← λ{λᵥ}

// se ← λ{v isa Symbol && v.name=b}

//===---------------------------===// repr //===---------------------------===//

String.prototype.repr_js ←! λ(){v ← this.valueOf(); return (v.match(~/'/g)||[]).length ≤ (v.match(~/"/g)||[]).length? //?
	"'"+seq(v).map(λ{{"'":"\\'",'\n':'\\n','\t':'\\t','\\':'\\\\'}ᵥ || (printable(v)? v : '\\u'+hex(ord(v),4))}).join('')+"'" :
	'"'+seq(v).map(λ{{'"':'\\"','\n':'\\n','\t':'\\t','\\':'\\\\'}ᵥ || (printable(v)? v : '\\u'+hex(ord(v),4))}).join('')+'"'}
RegExp.prototype.repr_js ←! λ{this+''}
String.prototype.repr ←! String.prototype.repr_js
RegExp.prototype.repr ←! λ{'~'+this+''}

//===------------------===// lexer: [char] → [token] //===-----------------===//

//! list of characters → list of tokens, which are symbols, self-evals, and spaces (symbols and self-evals need to know their [line,cell,col])

tokenize ← λ(s){
	seq(s).map(λ{printableᵥ ←! true})
	r ← []
	pos ← [1,1,1]; any ← undefined; first_type ← undefined
	spos ← λ(v){if (¬is(v.SPACE)) v.pos ←! pos; return v}
	increment_pos ← λ(s){
		seq(s).map(λ(v){
			if (v='\n') pos ←! [pos₀+1,1,1]
			else if (v='\t') pos ←! [pos₀,pos₁+1,1]
			else pos ←! [pos₀,pos₁,pos₂+1]
			}) }
	start_any ← λ(){any ←! s₀; s ←! s[1:]; first_type ←! js_valid_symbol.is_part(s₀)}
	end_any ← λ(){r.push(spos(new Symbol(any))); increment_pos(any); any ←! undefined}
	while (s≠'') {
		t ← reader_macros.map(λ(v){t←0; return (t←!s.match(v₀))? [t,v₁(t)] : undefined}).filter(λ{v})₀
		if (t) {m ← t₀; v ← t₁; if (is(any)) end_any(); r.push(spos(v)); s ←! s[m₀.length:]; increment_pos(m₀)}
		else {if (is(any)) {if (js_valid_symbol.is_part(s₀) = first_type) {any += s₀; s ←! s[1:]} else {end_any(); start_any()}} else start_any()}
	}
	pr('tokens:',r.map(λ{(v+'')[1:]}).join(' '))
	return r}

subscript_ops ← genex_2a(~/^-?[\da-z]$/).map(λ{seq(v).map(unicode.subscript)}).filter(λ{v.every(λ{v})}).map(λ{v.join('')})

// give tokens nice toString and inspect properties
SPACE ← {SPACE:''}; SPACE.toString ←! SPACE.inspect ←! λ{'`␣'}
Symbol ← λ(name){this.name ←! name}; Symbol.prototype.toString ←! Symbol.prototype.inspect ←! λ{'`'+this.name}
SelfEval ← λ(value){this.value ←! value}; SelfEval.prototype.toString ←! SelfEval.prototype.inspect ←! λ{'`'+this.value}

reader_macros ← [
[~/^(\/\/.*|\/\*[^]*?(\*\/|$)|[ \t\n\x0c\x0d])+/, λ{SPACE}],
[~/^(["'])((?:(?:.*[^\\])?(?:\\\\)*)?)\1/, λ{new SelfEval(v₂.match(~/\\u[\da-fA-F]{4}|\\x[\da-fA-F]{2}|\\.|./g).map(λ(v){return v.length>2? chr(parseInt(v[2:],16)) : v.length=2? v₁ : v}).join(''))}],
[~/^~\/((?:[^\/\\\[]|(?:\\.)|\[(?:[^\\\]]|(?:\\.))*\])*)\/([a-z]*)/, λ{new SelfEval(eval('/'+v₁+'/'+v₂))}],
[new RegExp('^('+[['~@','¬in'],seq('()[]{}‹›`~?:,;'),subscript_ops].m_concat().join('|').replace(~/([()\[\]{}?])/g,'\\$1')+')'), λ{new Symbol(v₁)}],
[~/^(0[xX][\da-fA-F]+|\d+[rR][\da-zA-Z]+|(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?)/, λ{new SelfEval(parseFloat(v₀))}]
]

//===-------===// token macros: [token] → macro-form-unresolved //===------===//

//! ---[1] we group tokens, which does the transformation:
// list of tokens → macro-form-with-unresolved-operators, which is a symbol, self-eval, space, or list of macro-form-with-unresolved-operators
// ---[2] we walk the form-tree and alias some tokens and do the transformations for new and λ, which leaves the datatype the same

//! CURRENT: implement token macros

token_macro_expand ← λ(tokens){
	err('tmx¬im')
}

// documentation from "precedence table.txt": \[…*\] (…*) {…*}        λ        new        ; && ||        \[ ( {
/*
special_expand ← λ(tokens){
	tokens.map(λ(v){if (v isa Symbol) v.name ←! aliases[v.name] || v.name})
	groups_expand ← λ(tokens){r ← group_expand(S('('),tokens.concat([S(')')])); if (¬r₁.no) err(r); return r₀}
		group_expand ← λ(g,l){
			check ← λ(){if (l.no) err('group is not closed @'+g.line)}
			check()
			r ← [g]; while(true){
				if (se(l₀,groups[g.name])) return [r,l[1:]]
				if (l₀ isa Symbol && own(groups,l₀.name)) {t ← (λ)(l₀,l[1:]); r.push(t₀); l ←! t₁; check(); continue}
				r.push(l₀); l ←! l[1:]; check()}}
	tokens ←! groups_expand(tokens)
	walk ← λ(v,f){return (λ(v,i,l,f){
		if (v isa Array) {for (j←0;j<v.length;) j ←! (λ)(v[j],j,v,f); return i+1}
		else return f(v,i,l) })(v,0,undefined,f)}
	walk(tokens,λ(v,i,l){
		if (se(v,'new') && l[i+1] isa Symbol) {l.splice(i,2,[v,[new Symbol('__QUOTE__','␣␣'),l[i+1]]]); return i+1}
		else if (se(v,'λ') && l[i+1] isa Array && se(l[i+1]₀,'(')) {l.splice(i,2,v,[new Symbol('__QUOTE__','␣␣'),l[i+1]]); return i+2}
		else if (se(v,'λ') && l[i+1] isa Array && se(l[i+1]₀,'{')) {l.splice(i,1,v,[new Symbol('__QUOTE__','␣␣'),S('v')]); return i+2}
		else return i+1 })
	return tokens}

groups ← ['()','[]','{}'].object()
aliases ← {';':',','&&':'&','||':'|'}
*/

//===-------===// operators: macro-form-unresolved → macro-form //===------===//

//! macro-form-with-unresolved-operators → macro-form, which is a symbol, self-eval, or list of macro-form

operator_expand ← λ(form){
	err('opx¬im')
}

/*
operators ← {} // {op_repr:op}
ANY_FORM ← {ANY:''} // object, not a dictionary
def_operator ← λ(prec_l,prec_r,op){
	op ←! op.match(~/[\[\].`_]|(\\.|[^\[\].`_])+/g)
	op ←! (λ(l){r ← []; while(true){
		if (l₀=']') return [r,l[1:]]
		if (l₀='[') {t ← (λ)(l[1:]); r.push(t₀); l ←! t₁; continue}
		r.push(l₀); l ←! l[1:]}})(op.concat([']']))₀
	op ←! op.map(λ{v isa Array? v.map(λ) : v='.'? ANY_FORM : v='_'? SPACE : v='`'? [SPACE] : v.replace(~/\\([_.`\[])/g,'$1')})

	left_eat ← 0; if (left_eat←!op₀=ANY_FORM) op ←! op[1:]
	//op.filter(λ{typeof(v)='string'})₀
	//print(op,left_eat)
	// ops.map
	// symbol_set_split ← λ(v){
	// 	t ← v.match(~/^([₋␣])?(.+?)([₋␣])?$/); p ← [t₁||'?',t₃||'?']; n ← t₂
	// 	return {name:n, spaces:λ{_.zip(p,v).every(λ{v₀='?'||v₀=v₁})}, symbols:λ(){me ← this; return ['₋␣','₋␣'].cartesian().map(λ{v.join('')}).filter(λ(v){return me.spaces(v)}).map(λ{new Symbol(me.name,v)})}}}
	// ops.map(λ(op){
	// 	op ←! op.match(~/\[.*?\]|..\[|[^\[\]]+/g).map(λ(v){t←0; r ← ((t ←! v₀='[' && v₋₁=']')? v[1:-1] : v).match(~/\.|[^.]+/g); return t?[r]:r}).m_concat()
	// 	op ←! op.map(λ{v isa Array? v.map(λ) : v='.'? op_ANY : v})
	// 	left_eat ← false; if (op₀=op_ANY) {left_eat ←! true; op ←! op[1:]}
	// 	t ← symbol_set_split(op₀); op ←! op[1:]
	// 	;({DOT:['.'], BACKTICK:['`'], SUBSCRIPT:subscript_ops}[t.name]||[t.name]).map(λ(n){
	// 		op ←! op.map(λ{v isa Array? v.map(λ) : v=op_ANY? v : symbol_set_split(v)})
	// 		r ← {prec_l:prec_l, prec_r:prec_r, left_eat:left_eat, op:_.defaults({name:n},t), right:op}
	// 		r.op.symbols().map(λ(v){operatorsᵥ ←! operatorsᵥ? (operatorsᵥ isa Array? operatorsᵥ : [operatorsᵥ]).concat([r]) : r})
	// 		}) }) }

;[	'400 401 .`\\.. .\\ᵥ .\\[ .(',
	'390 391 ¬. -.',
	'380 381 .*. ./. .%.',
	'370 371 .+. .-.',
	'350 350 .=. .≠. .<. .>. .≤. .≥.',
	'340 341 .&. .|.',
	'331 330 .?.:.',
	'321 322 .+=.',
	'323 320 .←. .←!.',
	'300 301 \\`. ~. ~@.',
	'290 291 ¬_.',
	'280 281 ._*_. ._/_. ._%_.',
	'270 271 ._+_. ._-_.',
	'260 260 ._isa_.',
	'250 250 ._=_. ._≠_. ._<_. ._>_. ._≤_. ._≥_.',
	'240 241 ._&_. ._|_.',
	'231 230 .?_._:_.',
	'221 222 ._+=_.',
	'223 220 ._←_. ._←!_.',
	'210 211 λ`.`. if`.[`:]`.[[`,]`else`.] while`.[`:]`. return[`.] try`.[[`,]`catch`.][[`,]`finally`.]',
	'200 201 \\`_. ~_. ~@_.',
	'100 100 , :'
	].map(λ(v){t ← v.split(' '); t[2:].map(λ(v){def_operator(i(t₀),i(t₁),v)})})
*/

//===-------------===// repr_js: macro-form → javascript //===-------------===//

//! we're done with our current architecture pass, so we cut it short super quickly by just having a list of "macros" that output javascript (probably javascript, maybe another layer to make it easy to output javascript)
// macro-form → repr_js

repr_js_file ← λ(form){
	err('rjf¬im')
}

//===--------------------------===// <edge> //===--------------------------===//

read ← λ(s){return pr(operator_expand(pr(token_macro_expand(tokenize(s)))))}
compile_f ← λ(_in,out){fs.writeFileSync(out,repr_js_file(read(fs.readFileSync(_in).toString())))}
compile_f(process.argv₂,process.argv₃)

//===-----------------------===// architecture //===-----------------------===//

/* okay. clarification / rethinking time.

[1] lexer (tokenize). (lexers are nice. lexers provide a simple, consistent tokenization.)
list of characters → list of tokens, which are symbols, self-evals, and spaces (symbols and self-evals need to know their [line,cell,col])

[2] arbitrary token manipulations (token macros). (this can be part of the lexer, but it's cleaner for it not to be)
we have two of these:
---[1] we group tokens, which does the transformation:
list of tokens → macro-form-with-unresolved-operators, which is a symbol, self-eval, space, or list of macro-form-with-unresolved-operators
---[2] we walk the form-tree and alias some tokens and do the transformations for new and λ, which leaves the datatype the same

[3] operators (operators). (this is a pretty encompassing type of parsing; most of the work gets done here. We could probably subsume token macros into this by extending this to full bnf.)
macro-form-with-unresolved-operators → macro-form, which is a symbol, self-eval, or list of macro-form

[4] we're done with our current architecture pass, so we cut it short super quickly by just having a list of "macros" that output javascript (probably javascript, maybe another layer to make it easy to output javascript)
macro-form → repr_js
*/

//===---------------------------===// todo //===---------------------------===//

/* ----------------- halt and catch fire notes ----------------
soooo i''m suddenly extremely concerned that maybe this whole ␣symbol₋ thing was a mistake. and that ".␣symbol₋." should be represented as ".␣symbol." and ".symbol." as ".␣?symbol␣?."

and ... are we actually fucking reinventing BNF? i think we just reinvented BNF. this is stupid.

"the role of the lexer is to take a stream of characters and generate a stream of symbols and literals and maybe spaces"
note: in js, 3. and .3 are parsed as numeric literals

prefix operator left precedence ?? should they always be at the top or something ??

space normal prefix operator in the middle ??

macro precedences?? thinking about numeric literals. and, um, maybe all literals. and isomorphisms. but maybe the stuff about the lexer makes this void.

we maybe want to get rid of the reader_or bull and just use sequential regexes
*/

// v.split(' ') might be the same concept as v[::' ']

// keep in mind that the basic idea was always just "write a really nice preprocessor" http://publications.gbdirect.co.uk/c_book/chapter7///how_the_preprocessor_works.html

// void could be a macro for undefined, or it could just be another global variable . i'm not sure what to do with such things that javascript doesn't like

// {expr,} = void
// {expr} = expr
// [1,,1] = [1 void 1]
// [1,1] = [1 1] = [1 1,] = [1 1]
// [1 1,,] = [1 1 void]
// [,1 1] = [void 1 1]

// try compiling an empty file

//"use strict"

// todo: [for] generator expression, non-lazy
// todo: (for) generator expression, lazy
// todo: {for} generator expression, set
// todo: {: for} generator expression, object
// todo?: v{} call
// todo: either (replace Ca with [] and [] with [__array__]) or (replace Ca with ())
// todo: curry can be max.(1) and indexer function can be list.[]

// todo: r ← [1 2 3]; r[1:2] ←! [5 5]; r = [1 5 5 3]
// todo: more meanings of ?

// later
// ` ~ ~@
//	tight					…++	…--
//	prefix					+…	bit~…
//	bitwise					bit|	bit^	bit&	bit>>>	bit>>	bit<<
//	assignment				+=	-=	*=	/=	&=	|=	bit<<=	bit>>=	bit>>>=	bit&=	bit^=	bit|=
//	statement				delete …	yield …

// derp, if ~/ is regex ~" should be too

// ¬in has to be a reader macro too ...

// fix the shit where all the functions are named λ

// actually yeah, → operator

// yeah, just have a bit() or bit{} anyfix namespace that provides ~ | ^ & >>> >> << ~= |= ^= &= >>>= >>= <<=

// i am starting to worry that we may have failed enormously by not studying perl

// _want_ syntax log₂n

// && || % are probably temporary
// add ++ -- with .₋\[
// add ␣+₋. with ␣-₋.
// add ./\ᵛ+/ with ./₋?\ᵥ/
// add -= *= /= &= |= with +=
// add back .in. .¬in. with isa
// ? bah, throw
// have a place for bitwise ops?
// what about `₋. ? dunno how i feel about that being like it is

/* PRECEDENCE_TABLE.TXT notes

[last item is full] [next item is left-wall]: recurse back up to whatever is eating many thing in sequence?
[last item is full] [next item is maybe left-wall]: ??? not sure ???
[last item is full] [next item is left-eaty]:
	if its left-priority > our right-priority: recurse to next item
	elif its left-priority < our right-priority: recurse upwards
	else: this is that =≠<>≤≥ thingy! or something like it!
[last item is full] [next item is void]: why marvelous, just let the above level handle this end
[last item is want] [next item is not-an-op]: eat it
[last item is want] [next item can be left-wall and have lower priority than us]: recurse to next item!
[last item is want] [next item is any other op]: error!
[last item is want] [next item is void]: oh no, you don''t get your satisfaction! but this is still not a problem.
[last item is void] [next item is left-wall]: marvelous, no problem, just do whatever the thing that is eating things says
[last item is void] [next item is maybe left-wall]: next item is definitely left-wall
[last item is void] [next item is left-eaty]: oh my well you don''t get to eat anything then! this is not actually a problem. we can just pass you void/undefined.
i think we can figure out left-priorities by just going on ahead and recursing to the item. because all of these things are pure! recursing to the item is cheap and stuff.

so it might be somewhat hard to figure out just what the priority of possibly even the last item is.
yeah. we didn''t solve the "which operator am i even?" problem.
alright. let''s leave that there for now and go ahead to implementing this. it looks like we have something actually solid now? i mean ... it''s still kinda just a conceptual prototype. but it looks like a _usable_ one

// add more than minimum? alpha precedence (so, 190 195 instead of 190 191? or just allow floats.)

// it might be worth considering the case of independent libraries.
// "fail-fast" could be adequate
